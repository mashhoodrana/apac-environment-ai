# Environmental Analysis Model Deployment Guide

This guide explains how to deploy the environmental prediction model and integrate it with the backend API.

## Overview

The system consists of:
1. A TensorFlow model trained on satellite data to predict environmental parameters
2. A backend API that serves the model predictions
3. A frontend that visualizes the predictions

## Model Deployment Options

### Option 1: Local Model Deployment

1. Export your trained model from Google Colab:
   ```python
   # In your Colab notebook
   model.save('/content/drive/MyDrive/env_model')
   ```

2. Download the model from Google Drive and place it in your project directory

3. Set the `MODEL_PATH` environment variable to point to your model:
   ```
   MODEL_PATH=/path/to/your/model
   ```

### Option 2: Using the Colab Notebook API

For development or when you want to use the model in Colab:

1. Enable the model service to access Colab by setting:
   ```
   COLAB_NOTEBOOK_URL=https://colab.research.google.com/drive/1G84mIHuly35rkdTutx2ldvLWnfkc1Aas
   ```

2. Make sure your Colab notebook has the prediction functions correctly implemented

3. No local model is required with this approach, as predictions are generated by the Colab notebook

### Option 3: Google Cloud AI Platform (Production)

For production deployment:

1. Deploy your model to Google Cloud AI Platform:
   ```bash
   # Package your model
   gcloud ai models upload \
     --region=us-central1 \
     --display-name=env-prediction-model \
     --artifact-uri=gs://your-bucket/model/
   ```

2. Update the model service to call the Google Cloud AI Platform API instead of using a local model

## Integration with the Backend

The model is integrated with the backend through `app/services/model_service.py`. This service:

1. Loads the model (if available locally)
2. Preprocesses input data
3. Makes predictions
4. Formats the results

## API Endpoints

The following API endpoints are available for model integration:

- `POST /api/analyze` - Analyze a location and include predictions (set `include_predictions=true`)
- `POST /api/predict` - Generate predictions for historical data or a specific location
- `POST /api/explain` - Get a human-readable explanation of the predictions

## Required Dependencies

Make sure your environment has all required dependencies installed:

```bash
pip install -r requirements.txt
```

Key dependencies:
- TensorFlow 2.15.0
- NumPy 1.26.0
- Pandas 2.1.1
- Scikit-learn 1.3.2

## Data Format

### Input Format

The model expects historical environmental data in this format:
```json
{
  "ndvi_values": [0.3, 0.32, 0.35, 0.33, 0.36, ...],
  "ndvi_dates": ["2023-01-01", "2023-01-02", ...],
  "temperature_values": [25.1, 24.8, 26.2, ...],
  "temperature_dates": ["2023-01-01", "2023-01-02", ...],
  "albedo_values": [0.2, 0.21, 0.22, ...],
  "albedo_dates": ["2023-01-01", "2023-01-02", ...]
}
```

### Output Format

The model returns predictions in this format:
```json
{
  "ndvi_prediction": {
    "dates": ["2023-02-01", "2023-02-02", ...],
    "values": [0.37, 0.38, 0.36, ...]
  },
  "temperature_prediction": {
    "dates": ["2023-02-01", "2023-02-02", ...],
    "values": [26.5, 27.1, 25.9, ...]
  },
  "albedo_prediction": {
    "dates": ["2023-02-01", "2023-02-02", ...],
    "values": [0.23, 0.22, 0.21, ...]
  }
}
```

## Troubleshooting

### Model Loading Issues

If you encounter issues loading the model:

1. Check that the `MODEL_PATH` environment variable is set correctly
2. Verify that TensorFlow is installed and working
3. Check the logs for specific error messages

### Prediction Issues

If predictions are not working as expected:

1. Verify the input data format matches what the model expects
2. Check preprocessing steps match how the model was trained
3. If using Colab, ensure the notebook is accessible and running

## Google Cloud Deployment

When deploying to Google Cloud:

1. Make sure the service account has access to the model artifacts
2. Set all required environment variables in the `app.yaml` file
3. For security, consider using Secret Manager for API keys and credentials 